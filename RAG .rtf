{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;\f2\fnil\fcharset0 Menlo-Regular;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid2\'02\'01.;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid101\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid201\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid301\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat4\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid401\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid501\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid601\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid702\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid801\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid9}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab560
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b\fs26 \cf0 Large Language Models:\
Slide - 1\
Intro To LLM\
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 Before entering into the topic I like to discuss about the LLM\'92s that you all are familiar with this form previous sessions on What is LLM\'92s? How do the work? And so on.\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 LLM\'92s is a type of artificial intelligence that can recognise and generate text, among other tasks. LLM\'92s are very large models that are pre-trained with vast amounts of data.\
A\'a0
\f0\b large language model
\f1\b0 \'a0is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques. Tasks like text generation, machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI are applications of the Large Language Model. Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional Encoder Representations from Transformers) by Google, etc.\
\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 Slide - 2
\f1\b0 \
Here I like to focus on challenges that we are facing in the LLM\'92s, there we have a lot of challenges in LLM\'92s, here we can discuss about\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	0.	}
\f0\b No Source(Hallucinations):
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	0.	}Hallucinations are noting but a sentence contradictions, prompt contradictions.\
{\listtext	0.	}This happens because of 
\f0\b data quality 
\f1\b0 like for example some of the LLM\'92s are trained by the 
\f0\b wikipedia
\f1\b0  or 
\f0\b reddit
\f1\b0  on which we are not sure the data in that are 100% correct or not.\
{\listtext	0.	}Also happens because of 
\f0\b Generation methods
\f1\b0  and 
\f0\b Input Contexts
\f1\b0  from the user-end(Should be clear). This should be minimise by shooting clear and specific prompts from the user. Also should made improvements in generation methods like 
\f0\b prompt engineering, function integration, retrieval-augmented generation(RAG), and fine-tuning etc...
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	0.	}
\f0\b Out of Date(Knowledge based problems)
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	0.	}We all are know how Generative AI technologies are powerful but they are limited by what they know. For example chatGPT or Bing they have a knowledge up to the year 2022. If we ask about any events that are takes place after 2022 then the LLM\'92s won\'92t have any idea about what we are asking.\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	0.	}
\f0\b Source Citations:
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	0.	}We don\'92t know what sources it is referring to generate a particular response. Also some time it presents false information when it doesn\'92t know the answer.\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	0.	}
\f0\b Longer\'a0 Training Time:\'a0
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	0.	}We know that how fast the data changes frequently day by day, to update/re-train llm\'92s with new information will take longer time which requires huge resources.\
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 \'a0\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 Slide - 3:
\f1\b0 \
To overcome the changes/limitations in llm\'92s there we have a various techniques like fine tuning, RAG, prompt engineering, function integration etc..\'a0\
So, RAG and fine tuning are most frequently used generation techniques to improve LLM\'92s.\
Lets discuss about RAG on how it is used for LLM\'92s to overcome the limitations in llm\'92s\
\'a0\

\f0\b Slide - 4
\f1\b0 \

\f0\b RAG (Retrieval Augmented Generation)
\f1\b0 \

\f0\b Introduction:-
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls2\ilvl0\cf0 {\listtext	0.	}RAG stands for 
\f0\b Retrieval Augmented Generation 
\f1\b0 which is introduced by Meta AI researchers
\f0\b .
\f1\b0 \
{\listtext	0.	}RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the 
\f0\b most accurate, up-to-date information
\f1\b0  and to give users insight into LLMs' generative process.\
{\listtext	0.	}It ensures that the model has access to the most current, reliable facts, and that users have access to the model\'92s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.\
{\listtext	0.	}By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or \'91hallucinate\'92 incorrect or misleading information.\
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 Slide - 5
\f1\b0 \

\f0\b How does RAG work?
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls3\ilvl0\cf0 {\listtext	0.	}Let us discuss about the generation, generation that generate a text in response to a user query, referred to as prompt. In LLM\'92s we have already seen undesirable behaviour like for example we discuss earlier about the source citation and presenting outdated info or false info confidently.\
{\listtext	0.	}In simple, User asks query to LLM, LLM will return a response confidently which is may be outdated``/highly confidential/false info. To overcome this issue here Retrieval Augmentation comes into the picture by adding a 
\f0\b content store 
\f1\b0 for instance
\f0\b  
\f1\b0 or whatever it contains data in the form of docs or what ever
\f0\b .
\f1\b0 \
{\listtext	0.	}Again user will ask the query, Now before giving response to the user LLM reaches to content store(which includes lots of data, collection docs, policies etc\'85) and 
\f0\b search
\f1\b0  in the content store which is relevant to users query it will provide the answer to the user.\
{\listtext	0.	}By this way limitations like outdate content, Hallucinations will overcome using RAG. If it doesn\'92t know the answer them LLM will directly say 
\f0\b \'93I don\'92t know\'94
\f1\b0  instead of false info\
{\listtext	0.	}Frameworks like 
\f0\b LangChain
\f1\b0  and 
\f0\b LlamaIndex
\f1\b0  have democratized RAG by making it possible to create simple knowledge-aware applications quickly.\
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 \'a0\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 \
Slide - 6
\f1\b0 \

\f0\b Building Blocks:-\
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 Slide - 7, 8
\f1\b0 \

\f0\b RAG using Atlas Vector Search:
\f1\b0 \

\f0\b Vector Embedding:
\f1\b0 \
Vector embedding is a sorting/describing things digitally(List of numbers or vectors). By using maths we can process them. This will make things easier like to search an items, chatting with AI, casting items\
\'a0\

\f0\b Example:
\f1\b0 \
Suppose we have fruits physically we can sort them by considering size or taste or colour etc.. In digital world, this fruits can be converted into digitally like a numbers or vectors in which vector embedding takes place.\
\'a0\

\f0\b Vector Search (Semantic Search):
\f1\b0 \
-> Vector search is used to find and retrieve information which is most relevant to given query.\
-> Vector search tries to understand the meaning or the context of the query.\
-> Vector search uses vector embeddings by 
\f0\b transforming
\f1\b0  both the 
\f0\b search query 
\f1\b0 and the i
\f0\b tems in the database
\f1\b0 (DB) like images, docs or products 
\f0\b in to vectors
\f1\b0 (digitalise). Then it started comparing those vectors to find best matches.  Finally it is providing a powerful tool for searching in large Data sets\
-> Vector embeddings in the form of [\'85, \'85., \'85, ]\

\f0\b Example:
\f1\b0 \
Query has 
\f0\b car
\f1\b0 \
Vector search may return result related to the tyres, driver or hybrid by understanding.\
\

\f0\b Slide - 9\
Orchestration:-\uc0\u8232 
\f1\b0 Combining RAG and orchestration is like giving your workflow management system a supercharged brain. It will help in various aspects like\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls4\ilvl0\cf0 {\listtext	0.	}
\f0\b Smart Decision making.\
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 		Orchestrating involves in making decisions based on available data. By integrating RAG models into the orchestration process these decisions can be made. This process contains wider range of knowledge. \
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 For example
\f1\b0 , a healthcare workflow can make more informed decisions by taking reference of the latest medical research, patient records, and best practices, thanks to RAG.\

\f0\b \
  2. Streamlined communication.\
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 		By using RAG to generate human-like text, systems can facilitate clearer and more understandable communication. For instance, in a customer support workflow, RAG can generate responses to customer queries that are not only accurate but also highly coherent and personalised.\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 \
  3. Continuous learning.\
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 	There we have a feedback loop between RAG and orchestration which is used to continuous improvements. Where the RAG models are fine tuned to learn from the outcomes of the orchestrated workflow.\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls5\ilvl0\cf0 {\listtext	0.	}
\f0\b Enhanced Documentation:\
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 	RAG models generated detailed reports and summaries, keeping a record of the entire workflow for auditing, compliance, or reference purposes. \
\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 Real World Applications:
\f1\b0 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls6\ilvl0\cf0 {\listtext	0.	}
\f0\b Customer Support
\f1\b0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 	
\f1\b0 RAG can empower customer support representatives with real-time access to product information, FAQs, and customer histories, improving response times and customer satisfaction\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls7\ilvl0\cf0 {\listtext	0.	}
\f0\b Legal and Financial Services
\f1\b0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 	Orchestrations can also manage the legal workflow flows as well from end to end and in terms of financial services RAG will generate accurate reports and also it will help in complex financial processess.\
\
Effective utilization of orchestration requires a deep understanding of RAG workflows.\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 \
Slide - 10\
Limitations of RAG\
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b\fs30 \cf0 LangChain
\f1\b0\fs26 \
LangChain is a framework designed to simplify the creation of LLM applications. It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. This allows AI developers to build LLM applications that leverage external sources of data (for example, private data sources).\
Some key things to note about LangChain:\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls8\ilvl0
\f2\fs18 \cf0 {\listtext	\uc0\u8226 	}
\f1\fs26 A \'93chain\'94 in LangChain is a sequence of components that can be combined together to solve a specific problem/perform a specific task.\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls8\ilvl1
\f2\fs18 \cf0 {\listtext	\uc0\u8226 	}
\f1\fs26 A chain can include components and modules such as wrappers for LLMs and vector stores, prompt templates, loaders, text splitting and chunking modules, retrievers, etc. Different components can be chained together.\uc0\u8232 \
\ls8\ilvl1
\f2\fs18 {\listtext	\uc0\u8226 	}
\f1\fs26 The chain takes the user's input and processes it through each component in the sequence.\uc0\u8232 \
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls8\ilvl0
\f2\fs18 \cf0 {\listtext	\uc0\u8226 	}
\f1\fs26 This modular approach of chaining different components or modules simplifies complex application development, debugging, and maintenance.\uc0\u8232 \
\ls8\ilvl0
\f2\fs18 {\listtext	\uc0\u8226 	}
\f1\fs26 LangChain is an open-source project launched in October 2022. The project has quickly gained popularity. It is getting a lot of adoption, including contributions from hundreds of developers on GitHub, and has an ever increasing number of integrations with external systems. It\'92s not only gaining a lot of adoption but also evolving rapidly to meet the needs of its growing user base.\
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f1\b0 \cf0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b \cf0 Building ChatBots using RAG:-
\f1\b0 \
RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the 
\f0\b most accurate, up-to-date information
\f1\b0  and to give users insight into LLMs' generative process.\
Implementing RAG in an LLM-based question answering system has two main benefits: \'a0\
\pard\pardeftab560\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	0.	}It ensures that the model has access to the most current, reliable facts, and that users have access to the model\'92s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.\
{\listtext	0.	}RAG has additional benefits. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or \'91hallucinate\'92 incorrect or misleading information.\
{\listtext	0.	}\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab560\slleading20\partightenfactor0

\f0\b\fs30 \cf0 Limitations in RAG:\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\
\
\
\
\
\
\
\
}